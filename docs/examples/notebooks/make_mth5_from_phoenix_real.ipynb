{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795ebd80-8a0f-479b-a888-161a5dfacbae",
   "metadata": {},
   "source": [
    "# Read Phoenix data into MTH5\n",
    "\n",
    "This example demonstrates how to read Phoenix data into an MTH5 file.  The data comes from example data in [PhoenixGeoPy](https://github.com/torresolmx/PhoenixGeoPy). Here I downloaded those data into a local folder on my computer by forking the main branch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74989d46-d13b-47e8-8189-17f515cc736a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5083c86-ebea-41d1-ade6-87e2de073b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 14:33:37,157 [line 135] mth5.setup_logger - INFO: Logging file can be found C:\\Users\\jpeacock\\OneDrive - DOI\\Documents\\GitHub\\mth5\\logs\\mth5_debug.log\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mth5.mth5 import MTH5\n",
    "from mth5 import read_file\n",
    "from mth5.io.phoenix import ReceiverMetadataJSON, PhoenixCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56259e6-b397-40c7-93e4-6ac30d1cdd68",
   "metadata": {},
   "source": [
    "## Data Directory\n",
    "\n",
    "Specify the station directory.  Phoenix files place each channel in a folder under the station directory named by the channel number.  There is also a `recmeta.json` file that has metadata output by the receiver that can be useful.  In the `PhoenixGeopPy/sample_data` there are 2 folders one for native data, these are `.bin` files which are the raw data in counts sampled at 24k.  There is also a folder for segmented files, these files are calibrated to millivolts and decimated or segmented data according to the recording configuration.  Most of the time you would use the segmented files? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6348263b-79f2-4bae-9981-cf4268d7a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_dir = Path(r\"c:\\Users\\jpeacock\\OneDrive - DOI\\mt\\phoenix_example_data\\10291_2019-09-06-015630\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce62cd7-97d7-4766-add3-1947cbd050df",
   "metadata": {},
   "source": [
    "## File Collection\n",
    "\n",
    "We've developed a collection dataframe to help sort out which files are which and which files can be grouped together into runs.  Continous runs will be given a single run name and segmented data will have sequential run names.  Both will have the pattern `sr{sample_rate}_####` for the `run.id`.\n",
    "\n",
    "#### Receiver Metadata\n",
    "\n",
    "The data logger or receiver will output a `JSON` file that contains useful metadata that is missing from the data files.  The `recmeta.json` file can be read into an object with methods to translate to `mt_metadata` objects. This is read in by `PhoenixCollection` and is in the attribute `receiver_metadata`. \n",
    "\n",
    "Here `PhoenixCollection.get_runs` returns a list of dataframes of just the first block within the sequence, as this is all you need for the reader.  For continous data the reader will read in all sequence blocks, for discontinous data it will only read in the burst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c61f11e-336f-4be1-88f2-2c2dda486f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_collection = PhoenixCollection(file_path=station_dir)\n",
    "run_list = phx_collection.get_runs(sample_rates=[150, 24000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97d19eb-6103-4468-b2bb-20c68a778cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_metadata = ReceiverMetadataJSON(station_dir.joinpath(r\"recmeta.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c310e9a-522c-453d-b230-eb704232cdb7",
   "metadata": {},
   "source": [
    "## Initiate MTH5\n",
    "\n",
    "First initiate an MTH5 file, can use the receiver metadata to fill in some `Survey` metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e44d4b-2173-4a9a-a6d7-f9335282eabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 14:33:52,525 [line 596] mth5.mth5.MTH5.open_mth5 - WARNING: mth5_from_phoenix.h5 will be overwritten in 'w' mode\n",
      "2022-08-23 14:33:52,913 [line 663] mth5.mth5.MTH5._initialize_file - INFO: Initialized MTH5 0.2.0 file c:\\Users\\jpeacock\\OneDrive - DOI\\mt\\phoenix_example_data\\10291_2019-09-06-015630\\mth5_from_phoenix.h5 in mode w\n"
     ]
    }
   ],
   "source": [
    "m = MTH5()\n",
    "m.open_mth5(station_dir.joinpath(\"mth5_from_phoenix.h5\"), \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1b671-69b1-4c14-8cd7-e9b73be0dcb1",
   "metadata": {},
   "source": [
    "### Add Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b628dba6-9f7f-48e5-9260-7e531fe37bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_metadata = phx_collection.receiver_metadata.survey_metadata\n",
    "survey_group = m.add_survey(survey_metadata.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd2943-4e0f-4b7e-8555-0ea10f5e822a",
   "metadata": {},
   "source": [
    "### Add Station\n",
    "\n",
    "Add a station and station metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e57557-8fc2-42db-b8be-f6517650cf4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_metadata = phx_collection.receiver_metadata.station_metadata\n",
    "station_group = survey_group.stations_group.add_station(\n",
    "    station_metadata.id, \n",
    "    station_metadata=station_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5bd8d-c10f-42f5-9a85-45332d92f48b",
   "metadata": {},
   "source": [
    "## Loop through runs\n",
    "\n",
    "Using the `run_list` value output by the `PhoenixCollection.get_runs` we can simply loop through the runs without knowing if the data are continous or discontinuous, the `read_file` will take care of that.\n",
    "\n",
    "Users should note the Phoenix file structure.  Inside the folder are files with extensions of `.td_24k` and `td_150`.  \n",
    "\n",
    "- `.td_24k` are usually bursts of a few seconds of data sampled at 24k samples per second to get high frequency information.  The returned object is a `mth5.timeseries.ChannelTS`.\n",
    "- `td_150` is data continuously sampled at 150 samples per second.  These files usually have a set length, commonly an hour. The returned object is a `mth5.timeseries.ChannelTS`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ddfb1-7bb3-4606-8362-170eb1edaa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 14:36:15,506 [line 784] mth5.groups.base.Station.add_run - INFO: run sr150_0001 already exists, returning existing group.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for run_df in run_list:\n",
    "    run_metadata = phx_collection.receiver_metadata.run_metadata\n",
    "    run_metadata.id = run_df.run.unique()[0]\n",
    "    run_metadata.sample_rate = float(run_df.sample_rate.unique()[0])\n",
    "    \n",
    "    run_group = station_group.add_run(run_metadata.id, run_metadata=run_metadata)\n",
    "    for row in run_df.itertuples():\n",
    "        ch_ts = read_file(row.fn, **{\"channel_map\":phx_collection.receiver_metadata.channel_map})\n",
    "        ch_metadata = phx_collection.receiver_metadata.get_ch_metadata(\n",
    "            ch_ts.channel_metadata.channel_number\n",
    "        )\n",
    "        # need to update the time period as estimated from the data not the metadata\n",
    "        ch_metadata.time_period.update(ch_ts.channel_metadata.time_period)\n",
    "        ch_ts.channel_metadata.update(ch_metadata)\n",
    "        ch_dataset = run_group.from_channel_ts(ch_ts)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223f5c8-3c5a-42c1-98de-1e4897dff91a",
   "metadata": {},
   "source": [
    "#### Add a Run for continuous data\n",
    "\n",
    "Here we will add a run for the continuous data labelled `sr150_0001`.  This is just a suggestion, you could name it whatever makes sense to you. We will use the the collection to make continuous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd542c9-6dd6-4aaa-a8de-46b6230844e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_20460\\297519762.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_150\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mruns_150\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_150\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrun_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mruns_150\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mrun_df_150\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_150\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_150\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfirst_block\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_df_150\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrun_df_150\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrun_df_150\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_150 = df.loc[df.sample_rate == 150]\n",
    "runs_150 = df_150.run.unique()\n",
    "for run_id in runs_150:\n",
    "    run_df_150 = df_150.loc[df_150.run == run_id]\n",
    "    first_block = run_df_150.loc[run_df_150.start == run_df_150.start.min()]\n",
    "    \n",
    "    run_metadata = phx_collection.receiver_metadata.run_metadata\n",
    "    run_metadata.id = run_id\n",
    "    run_metadata.sample_rate = 150.\n",
    "    continuous_run = station_group.add_run(run_metadata.id, run_metadata=run_metadata)\n",
    "    for fn in df_150.loc[df_150.start == df_150.start.min()].fn:\n",
    "        ch_150_ts = read_file(fn, **{\"channel_map\":phx_collection.receiver_metadata.channel_map})\n",
    "        ch_metadata = phx_collection.receiver_metadata.get_ch_metadata(\n",
    "            ch_150_ts.channel_metadata.channel_number\n",
    "        )\n",
    "        # need to update the time period as estimated from the data not the metadata\n",
    "        ch_metadata.time_period.update(ch_150_ts.channel_metadata.time_period)\n",
    "        ch_150_ts.channel_metadata.update(ch_metadata)\n",
    "        ch_dataset = continuous_run.from_channel_ts(ch_150_ts)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720a2d2-e662-4a22-a2e7-2534ad84badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metadata = phx_collection.receiver_metadata.run_metadata\n",
    "run_metadata.id = \"sr150_001\"\n",
    "run_metadata.sample_rate = 150.\n",
    "continuous_run = station_group.add_run(run_metadata.id, run_metadata=run_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6d0bf-2940-4eb4-bd45-b7f89c1f512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ch_dir in station_dir.iterdir():\n",
    "    if ch_dir.is_dir():\n",
    "        ch_metadata = phx_collection.receiver_metadata.get_ch_metadata(int(ch_dir.stem))\n",
    "        # need to set sample rate to 0 so it does not override existing value\n",
    "        ch_metadata.sample_rate = 0\n",
    "        ch_150 = read_file(\n",
    "            sorted(list(ch_dir.glob(\"*.td_150\")))[0],\n",
    "            **{\"channel_map\":phx_collection.receiver_metadata.channel_map}\n",
    "        )\n",
    "        # need to update the time period as estimated from the data not the metadata\n",
    "        ch_metadata.time_period.update(ch_150.channel_metadata.time_period)\n",
    "        ch_150.channel_metadata.update(ch_metadata)\n",
    "        ch_dataset = continuous_run.from_channel_ts(ch_150)\n",
    "        \n",
    "continuous_run.validate_run_metadata()\n",
    "continuous_run.write_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2923c-aa99-4769-9a41-17d0c0a1f1a5",
   "metadata": {},
   "source": [
    "#### Update metadata before closing\n",
    "\n",
    "Need to update the metadata to account for added stations, runs, and channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9318c-d595-4a21-ba86-e44b055282b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "station_group.validate_station_metadata()\n",
    "station_group.write_metadata()\n",
    "\n",
    "survey_group.update_survey_metadata()\n",
    "survey_group.write_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebf141-0dc4-4385-aecf-94d279fe1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.channel_summary.summarize()\n",
    "m.channel_summary.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6acfb-924d-4ceb-b15f-7f7deb7028be",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.close_mth5()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
